{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import time\n",
    "\n",
    "from config import bpr_config\n",
    "from data_utils import RecDataset, DataProvider\n",
    "from evaluation.rec_evaluator import RecEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = bpr_config.epochs\n",
    "batch_size = bpr_config.batch_size\n",
    "emb_dim = bpr_config.emb_dim\n",
    "device = bpr_config.device\n",
    "eta = bpr_config.eta\n",
    "weight_decay = bpr_config.weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters and datset-specific parameters\n",
    "rec_dataset = RecDataset(bpr_config.dir_path)\n",
    "all_users = rec_dataset.get_users()\n",
    "all_items = rec_dataset.get_items()\n",
    "num_users = rec_dataset.get_num_users()\n",
    "num_items = rec_dataset.get_num_items()\n",
    "bought_mask = rec_dataset.get_bought_mask().to(device)\n",
    "eval_dict = rec_dataset.get_interaction_records(\"test\")\n",
    "bought_dict = rec_dataset.get_interaction_records()\n",
    "train_ui = rec_dataset.get_user_item_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DataProvider(device)\n",
    "evaluator = RecEvaluator(eval_dict, None, device)\n",
    "writer = SummaryWriter(\"runs/BPR-Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPR(nn.Module):\n",
    "    '''\n",
    "     BPR Model\n",
    "    '''\n",
    "    def __init__(self, num_users, num_items, emb_dim, bought_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_users: The number of users.\n",
    "            num_items: The number of items.\n",
    "            emb_dim: Embedding dimension of embedding layer.\n",
    "        \"\"\"\n",
    "        super(BPR, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_dim)\n",
    "        self.bought_mask = bought_mask\n",
    "        nn.init.normal_(self.user_emb.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_emb.weight, std=0.01)\n",
    "        \n",
    "    def loss(self, users, pos_items, neg_items):\n",
    "        \n",
    "        emb_users = self.user_emb(users)\n",
    "        emb_pos_items = self.item_emb(pos_items)\n",
    "        emb_neg_items = self.item_emb(neg_items)\n",
    "        x_ui = torch.sum(emb_users * emb_pos_items, 1)\n",
    "        x_uj = torch.sum(emb_users * emb_neg_items, 1)\n",
    "        x_uij = x_ui - x_uj\n",
    "        log_prob = torch.log(torch.sigmoid(x_uij)).mean()\n",
    "        return -log_prob\n",
    "    \n",
    "    def forward(self, users, k = 10, delete_bought = True):\n",
    "        return self.top_k_items_for_users(users, k, delete_bought)\n",
    "        users_emb = self.user_emb(users)\n",
    "        items_emb = self.item_emb(self.items)\n",
    "        scores = torch.mm(users_emb, items_emb.t())\n",
    "        scores = torch.sigmoid(scores)\n",
    "        if delete_bought:\n",
    "            scores[self.bought_mask[users].bool()] = scores.min()-1\n",
    "        _,indices = torch.topk(scores, k, dim=1)\n",
    "        return self.items[indices]\n",
    "    \n",
    "    def top_k_items_for_users(self, users, k = 5, delete_bought = True):\n",
    "        \"\"\" Gets top k items for users.\n",
    "        \n",
    "        Args:\n",
    "            users: Target users.\n",
    "            k: The number of items to recommend for each user.\n",
    "            delete_bought: Boolean indicating whether recommend items bought before\n",
    "        \n",
    "        Returns:\n",
    "            A tensor containing top k items for target users.\n",
    "        \"\"\"\n",
    "        \n",
    "        users_emb = self.user_emb(users)\n",
    "        items_emb = self.item_emb.weight\n",
    "        scores = torch.mm(users_emb, items_emb.t())\n",
    "        scores = torch.sigmoid(scores)\n",
    "        if delete_bought:\n",
    "            scores[self.bought_mask[users].bool()] = scores.min()-1\n",
    "        _, top_k_items = torch.topk(scores, k)\n",
    "        return top_k_items\n",
    "    \n",
    "    def rank_items_for_users(self, users, items=None):  \n",
    "        '''\n",
    "            For each user we have many candidate items, rank them based on scores.\n",
    "        '''\n",
    "        num_users = users.size()[0]\n",
    "        if(items is None):\n",
    "            items = self.items.repeat(num_users,1)\n",
    "        num_items = items.size()[1]\n",
    "        items_emb = self.item_emb(items)\n",
    "        users_emb = self.user_emb(users).view(num_users,1,-1).repeat(1,num_items,1)\n",
    "        scores = torch.mul(users_emb,items_emb).sum(2)\n",
    "        scores = torch.sigmoid(scores)\n",
    "        _,items_ind = torch.sort(scores, 1, descending=True)\n",
    "        return torch.gather(items,1,items_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BPR(num_users, num_items, emb_dim, bought_mask)\n",
    "model = model.to(device)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=eta, momentum = 0.9, weight_decay = weight_decay)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=eta, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[Epochs 1/200] [epoch loss: 478.24875] [Time:2.27315]\n",
      "\t[Epochs 2/200] [epoch loss: 473.61350] [Time:2.90516]\n",
      "\t[Epochs 3/200] [epoch loss: 451.62170] [Time:2.90628]\n",
      "\t[Epochs 4/200] [epoch loss: 411.18129] [Time:2.35845]\n",
      "\t[Epochs 5/200] [epoch loss: 364.65451] [Time:2.21662]\n",
      "\t[Epochs 6/200] [epoch loss: 327.26519] [Time:2.62271]\n",
      "\t[Epochs 7/200] [epoch loss: 296.45812] [Time:2.25556]\n",
      "\t[Epochs 8/200] [epoch loss: 274.53698] [Time:2.28421]\n",
      "\t[Epochs 9/200] [epoch loss: 259.13092] [Time:2.19936]\n",
      "\t[Epochs 10/200] [epoch loss: 248.15015] [Time:2.40050]\n",
      "\t[Epochs 11/200] [epoch loss: 247.17495] [Time:2.52729]\n",
      "\t[Epochs 12/200] [epoch loss: 239.92952] [Time:2.45936]\n",
      "\t[Epochs 13/200] [epoch loss: 234.43255] [Time:2.75039]\n",
      "\t[Epochs 14/200] [epoch loss: 230.03656] [Time:2.98786]\n",
      "\t[Epochs 15/200] [epoch loss: 226.37882] [Time:3.09285]\n",
      "\t[Epochs 16/200] [epoch loss: 228.08649] [Time:2.48400]\n",
      "\t[Epochs 17/200] [epoch loss: 222.93125] [Time:2.41042]\n",
      "\t[Epochs 18/200] [epoch loss: 218.97714] [Time:2.43231]\n",
      "\t[Epochs 19/200] [epoch loss: 215.58953] [Time:2.43465]\n",
      "\t[Epochs 20/200] [epoch loss: 212.50544] [Time:2.51404]\n",
      "\t[Epochs 21/200] [epoch loss: 220.80175] [Time:3.17261]\n",
      "\t[Epochs 22/200] [epoch loss: 215.65983] [Time:2.98571]\n",
      "\t[Epochs 23/200] [epoch loss: 211.63507] [Time:2.94624]\n",
      "\t[Epochs 24/200] [epoch loss: 208.06807] [Time:2.95587]\n",
      "\t[Epochs 25/200] [epoch loss: 204.71703] [Time:2.96114]\n",
      "\t[Epochs 26/200] [epoch loss: 209.60759] [Time:2.51061]\n",
      "\t[Epochs 27/200] [epoch loss: 204.08747] [Time:2.49675]\n",
      "\t[Epochs 28/200] [epoch loss: 199.60318] [Time:2.45767]\n",
      "\t[Epochs 29/200] [epoch loss: 195.58766] [Time:2.92417]\n",
      "\t[Epochs 30/200] [epoch loss: 191.83640] [Time:2.93751]\n",
      "\t[Epochs 31/200] [epoch loss: 200.49437] [Time:2.59456]\n",
      "\t[Epochs 32/200] [epoch loss: 194.80801] [Time:2.49534]\n",
      "\t[Epochs 33/200] [epoch loss: 190.20578] [Time:2.40987]\n",
      "\t[Epochs 34/200] [epoch loss: 186.12147] [Time:2.37928]\n",
      "\t[Epochs 35/200] [epoch loss: 182.35294] [Time:2.69979]\n",
      "\t[Epochs 36/200] [epoch loss: 191.25470] [Time:3.19303]\n",
      "\t[Epochs 37/200] [epoch loss: 185.52174] [Time:2.95973]\n",
      "\t[Epochs 38/200] [epoch loss: 180.87427] [Time:2.94622]\n",
      "\t[Epochs 39/200] [epoch loss: 176.76908] [Time:2.99599]\n",
      "\t[Epochs 40/200] [epoch loss: 173.00807] [Time:3.02365]\n",
      "\t[Epochs 41/200] [epoch loss: 179.51731] [Time:3.00687]\n",
      "\t[Epochs 42/200] [epoch loss: 174.12982] [Time:2.98005]\n",
      "\t[Epochs 43/200] [epoch loss: 169.74674] [Time:3.00963]\n",
      "\t[Epochs 44/200] [epoch loss: 165.89366] [Time:2.98837]\n",
      "\t[Epochs 45/200] [epoch loss: 162.39323] [Time:2.95497]\n",
      "\t[Epochs 46/200] [epoch loss: 170.74454] [Time:3.28875]\n",
      "\t[Epochs 47/200] [epoch loss: 165.60859] [Time:3.41121]\n",
      "\t[Epochs 48/200] [epoch loss: 161.46247] [Time:3.01319]\n",
      "\t[Epochs 49/200] [epoch loss: 157.84659] [Time:2.50694]\n",
      "\t[Epochs 50/200] [epoch loss: 154.58782] [Time:2.50421]\n",
      "\t[Epochs 51/200] [epoch loss: 164.75909] [Time:2.66162]\n",
      "\t[Epochs 52/200] [epoch loss: 159.57922] [Time:2.95514]\n",
      "\t[Epochs 53/200] [epoch loss: 155.44560] [Time:3.00312]\n",
      "\t[Epochs 54/200] [epoch loss: 151.88000] [Time:3.01147]\n",
      "\t[Epochs 55/200] [epoch loss: 148.69845] [Time:2.97084]\n",
      "\t[Epochs 56/200] [epoch loss: 157.04376] [Time:3.02773]\n",
      "\t[Epochs 57/200] [epoch loss: 152.17702] [Time:3.01675]\n",
      "\t[Epochs 58/200] [epoch loss: 148.31597] [Time:3.02426]\n",
      "\t[Epochs 59/200] [epoch loss: 144.99211] [Time:2.99526]\n",
      "\t[Epochs 60/200] [epoch loss: 142.02927] [Time:3.01085]\n",
      "\t[Epochs 61/200] [epoch loss: 154.16428] [Time:3.07340]\n",
      "\t[Epochs 62/200] [epoch loss: 149.19592] [Time:2.92370]\n",
      "\t[Epochs 63/200] [epoch loss: 145.29970] [Time:2.85120]\n",
      "\t[Epochs 64/200] [epoch loss: 141.97577] [Time:3.00638]\n",
      "\t[Epochs 65/200] [epoch loss: 139.03461] [Time:2.99449]\n",
      "\t[Epochs 66/200] [epoch loss: 149.28160] [Time:3.09714]\n",
      "\t[Epochs 67/200] [epoch loss: 144.46665] [Time:2.93680]\n",
      "\t[Epochs 68/200] [epoch loss: 140.68558] [Time:3.21087]\n",
      "\t[Epochs 69/200] [epoch loss: 137.45450] [Time:3.18000]\n",
      "\t[Epochs 70/200] [epoch loss: 134.59349] [Time:2.99620]\n",
      "\t[Epochs 71/200] [epoch loss: 149.88313] [Time:2.35390]\n",
      "\t[Epochs 72/200] [epoch loss: 144.83346] [Time:2.10679]\n",
      "\t[Epochs 73/200] [epoch loss: 140.92892] [Time:2.09461]\n",
      "\t[Epochs 74/200] [epoch loss: 137.63167] [Time:2.06310]\n",
      "\t[Epochs 75/200] [epoch loss: 134.74101] [Time:1.94506]\n",
      "\t[Epochs 76/200] [epoch loss: 145.56224] [Time:2.32140]\n",
      "\t[Epochs 77/200] [epoch loss: 140.51460] [Time:2.09981]\n",
      "\t[Epochs 78/200] [epoch loss: 136.59812] [Time:2.06936]\n",
      "\t[Epochs 79/200] [epoch loss: 133.28516] [Time:2.06553]\n",
      "\t[Epochs 80/200] [epoch loss: 130.37765] [Time:2.17631]\n",
      "\t[Epochs 81/200] [epoch loss: 142.82112] [Time:3.06882]\n",
      "\t[Epochs 82/200] [epoch loss: 137.72877] [Time:2.96379]\n",
      "\t[Epochs 83/200] [epoch loss: 133.76848] [Time:2.95986]\n",
      "\t[Epochs 84/200] [epoch loss: 130.41199] [Time:2.99260]\n",
      "\t[Epochs 85/200] [epoch loss: 127.46136] [Time:3.00947]\n",
      "\t[Epochs 86/200] [epoch loss: 140.82901] [Time:2.64899]\n",
      "\t[Epochs 87/200] [epoch loss: 135.88888] [Time:2.45254]\n",
      "\t[Epochs 88/200] [epoch loss: 132.06748] [Time:2.45161]\n",
      "\t[Epochs 89/200] [epoch loss: 128.83982] [Time:2.49458]\n",
      "\t[Epochs 90/200] [epoch loss: 126.00994] [Time:2.44122]\n",
      "\t[Epochs 91/200] [epoch loss: 140.22000] [Time:3.03667]\n",
      "\t[Epochs 92/200] [epoch loss: 135.09052] [Time:2.48113]\n",
      "\t[Epochs 93/200] [epoch loss: 131.12802] [Time:2.51002]\n",
      "\t[Epochs 94/200] [epoch loss: 127.78810] [Time:2.49072]\n",
      "\t[Epochs 95/200] [epoch loss: 124.86720] [Time:2.46860]\n",
      "\t[Epochs 96/200] [epoch loss: 136.05852] [Time:3.09676]\n",
      "\t[Epochs 97/200] [epoch loss: 131.12307] [Time:3.01628]\n",
      "\t[Epochs 98/200] [epoch loss: 127.27602] [Time:3.04908]\n",
      "\t[Epochs 99/200] [epoch loss: 124.01696] [Time:2.99494]\n",
      "\t[Epochs 100/200] [epoch loss: 121.15796] [Time:3.03781]\n",
      "\t[Epochs 101/200] [epoch loss: 135.14839] [Time:3.07186]\n",
      "\t[Epochs 102/200] [epoch loss: 130.02840] [Time:3.00917]\n",
      "\t[Epochs 103/200] [epoch loss: 126.07517] [Time:2.94416]\n",
      "\t[Epochs 104/200] [epoch loss: 122.74489] [Time:2.78027]\n",
      "\t[Epochs 105/200] [epoch loss: 119.83593] [Time:3.00791]\n",
      "\t[Epochs 106/200] [epoch loss: 138.07667] [Time:2.99664]\n",
      "\t[Epochs 107/200] [epoch loss: 132.90034] [Time:2.84378]\n",
      "\t[Epochs 108/200] [epoch loss: 128.94472] [Time:2.98002]\n",
      "\t[Epochs 109/200] [epoch loss: 125.63163] [Time:3.00572]\n",
      "\t[Epochs 110/200] [epoch loss: 122.75154] [Time:2.95341]\n",
      "\t[Epochs 111/200] [epoch loss: 134.50307] [Time:3.37556]\n",
      "\t[Epochs 112/200] [epoch loss: 129.36195] [Time:3.36386]\n",
      "\t[Epochs 113/200] [epoch loss: 125.39019] [Time:3.35609]\n",
      "\t[Epochs 114/200] [epoch loss: 122.04741] [Time:2.75240]\n",
      "\t[Epochs 115/200] [epoch loss: 119.13454] [Time:2.42712]\n",
      "\t[Epochs 116/200] [epoch loss: 132.57666] [Time:3.08216]\n",
      "\t[Epochs 117/200] [epoch loss: 127.46762] [Time:3.01846]\n",
      "\t[Epochs 118/200] [epoch loss: 123.54901] [Time:3.00556]\n",
      "\t[Epochs 119/200] [epoch loss: 120.27400] [Time:3.00650]\n",
      "\t[Epochs 120/200] [epoch loss: 117.43879] [Time:3.00081]\n",
      "\t[Epochs 121/200] [epoch loss: 130.47723] [Time:3.46763]\n",
      "\t[Epochs 122/200] [epoch loss: 125.25678] [Time:3.42537]\n",
      "\t[Epochs 123/200] [epoch loss: 121.23024] [Time:3.04680]\n",
      "\t[Epochs 124/200] [epoch loss: 117.85321] [Time:3.12659]\n",
      "\t[Epochs 125/200] [epoch loss: 114.92342] [Time:3.00916]\n",
      "\t[Epochs 126/200] [epoch loss: 130.15595] [Time:3.10449]\n",
      "\t[Epochs 127/200] [epoch loss: 125.03920] [Time:2.94753]\n",
      "\t[Epochs 128/200] [epoch loss: 121.12513] [Time:2.99296]\n",
      "\t[Epochs 129/200] [epoch loss: 117.85212] [Time:2.96094]\n",
      "\t[Epochs 130/200] [epoch loss: 115.01701] [Time:2.97571]\n",
      "\t[Epochs 131/200] [epoch loss: 129.60861] [Time:3.03040]\n",
      "\t[Epochs 132/200] [epoch loss: 124.58730] [Time:2.98844]\n",
      "\t[Epochs 133/200] [epoch loss: 120.75306] [Time:2.94765]\n",
      "\t[Epochs 134/200] [epoch loss: 117.54770] [Time:2.93873]\n",
      "\t[Epochs 135/200] [epoch loss: 114.76833] [Time:2.97759]\n",
      "\t[Epochs 136/200] [epoch loss: 130.89816] [Time:2.97505]\n",
      "\t[Epochs 137/200] [epoch loss: 125.63105] [Time:2.74476]\n",
      "\t[Epochs 138/200] [epoch loss: 121.62482] [Time:2.86841]\n",
      "\t[Epochs 139/200] [epoch loss: 118.28322] [Time:2.97986]\n",
      "\t[Epochs 140/200] [epoch loss: 115.39270] [Time:3.02723]\n",
      "\t[Epochs 141/200] [epoch loss: 128.69609] [Time:2.64809]\n",
      "\t[Epochs 142/200] [epoch loss: 123.68515] [Time:2.90037]\n",
      "\t[Epochs 143/200] [epoch loss: 119.82939] [Time:2.91439]\n",
      "\t[Epochs 144/200] [epoch loss: 116.59222] [Time:3.09424]\n",
      "\t[Epochs 145/200] [epoch loss: 113.78189] [Time:3.07921]\n",
      "\t[Epochs 146/200] [epoch loss: 126.55295] [Time:2.53429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[Epochs 147/200] [epoch loss: 121.38925] [Time:3.00074]\n",
      "\t[Epochs 148/200] [epoch loss: 117.44673] [Time:2.93821]\n",
      "\t[Epochs 149/200] [epoch loss: 114.15843] [Time:2.98322]\n",
      "\t[Epochs 150/200] [epoch loss: 111.31742] [Time:2.69824]\n",
      "\t[Epochs 151/200] [epoch loss: 127.24883] [Time:2.58521]\n",
      "\t[Epochs 152/200] [epoch loss: 122.14845] [Time:2.38126]\n",
      "\t[Epochs 153/200] [epoch loss: 118.26262] [Time:2.47794]\n",
      "\t[Epochs 154/200] [epoch loss: 115.02007] [Time:2.52003]\n",
      "\t[Epochs 155/200] [epoch loss: 112.21620] [Time:2.48532]\n",
      "\t[Epochs 156/200] [epoch loss: 128.04656] [Time:2.96758]\n",
      "\t[Epochs 157/200] [epoch loss: 122.82951] [Time:2.83874]\n",
      "\t[Epochs 158/200] [epoch loss: 118.87540] [Time:3.39596]\n",
      "\t[Epochs 159/200] [epoch loss: 115.59406] [Time:3.29333]\n",
      "\t[Epochs 160/200] [epoch loss: 112.77213] [Time:2.92961]\n",
      "\t[Epochs 161/200] [epoch loss: 127.57106] [Time:3.11313]\n",
      "\t[Epochs 162/200] [epoch loss: 122.35566] [Time:2.97733]\n",
      "\t[Epochs 163/200] [epoch loss: 118.38934] [Time:3.00661]\n",
      "\t[Epochs 164/200] [epoch loss: 115.09078] [Time:2.99877]\n",
      "\t[Epochs 165/200] [epoch loss: 112.24968] [Time:2.99289]\n",
      "\t[Epochs 166/200] [epoch loss: 130.79058] [Time:3.10409]\n",
      "\t[Epochs 167/200] [epoch loss: 125.42292] [Time:2.99456]\n",
      "\t[Epochs 168/200] [epoch loss: 121.38107] [Time:3.03720]\n",
      "\t[Epochs 169/200] [epoch loss: 118.04908] [Time:2.96956]\n",
      "\t[Epochs 170/200] [epoch loss: 115.20159] [Time:2.99988]\n",
      "\t[Epochs 171/200] [epoch loss: 130.31069] [Time:3.09292]\n",
      "\t[Epochs 172/200] [epoch loss: 124.98304] [Time:2.99596]\n",
      "\t[Epochs 173/200] [epoch loss: 120.93496] [Time:2.98823]\n",
      "\t[Epochs 174/200] [epoch loss: 117.56537] [Time:3.01700]\n",
      "\t[Epochs 175/200] [epoch loss: 114.65927] [Time:2.99298]\n",
      "\t[Epochs 176/200] [epoch loss: 129.87280] [Time:3.10284]\n",
      "\t[Epochs 177/200] [epoch loss: 124.53260] [Time:3.12725]\n",
      "\t[Epochs 178/200] [epoch loss: 120.43601] [Time:2.98104]\n",
      "\t[Epochs 179/200] [epoch loss: 117.01554] [Time:3.03790]\n",
      "\t[Epochs 180/200] [epoch loss: 114.06353] [Time:2.98884]\n",
      "\t[Epochs 181/200] [epoch loss: 127.64809] [Time:3.13912]\n",
      "\t[Epochs 182/200] [epoch loss: 122.38852] [Time:3.01459]\n",
      "\t[Epochs 183/200] [epoch loss: 118.38516] [Time:3.04360]\n",
      "\t[Epochs 184/200] [epoch loss: 115.06091] [Time:3.01180]\n",
      "\t[Epochs 185/200] [epoch loss: 112.20420] [Time:2.97979]\n",
      "\t[Epochs 186/200] [epoch loss: 128.47806] [Time:3.08467]\n",
      "\t[Epochs 187/200] [epoch loss: 123.15613] [Time:2.99514]\n",
      "\t[Epochs 188/200] [epoch loss: 119.12173] [Time:2.98010]\n",
      "\t[Epochs 189/200] [epoch loss: 115.77760] [Time:3.02270]\n",
      "\t[Epochs 190/200] [epoch loss: 112.90572] [Time:3.01958]\n",
      "\t[Epochs 191/200] [epoch loss: 128.95034] [Time:3.05576]\n",
      "\t[Epochs 192/200] [epoch loss: 123.61394] [Time:2.99362]\n",
      "\t[Epochs 193/200] [epoch loss: 119.56433] [Time:2.91629]\n",
      "\t[Epochs 194/200] [epoch loss: 116.20423] [Time:3.00710]\n",
      "\t[Epochs 195/200] [epoch loss: 113.31555] [Time:2.97678]\n",
      "\t[Epochs 196/200] [epoch loss: 127.56186] [Time:2.86923]\n",
      "\t[Epochs 197/200] [epoch loss: 122.32899] [Time:2.36918]\n",
      "\t[Epochs 198/200] [epoch loss: 118.34428] [Time:2.20826]\n",
      "\t[Epochs 199/200] [epoch loss: 115.02659] [Time:2.90166]\n",
      "\t[Epochs 200/200] [epoch loss: 112.16676] [Time:2.91743]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    time_start = time.time()\n",
    "    if(epoch %5 == 0):\n",
    "        train_data = dp.prepare_bpr_triplets(all_items, bought_dict, bpr_config.batch_size)\n",
    "    loss_epoch = 0\n",
    "    for users, pos_items, neg_items in train_data:\n",
    "        loss = model.loss(users,pos_items,neg_items)\n",
    "        loss_epoch += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    time_end = time.time()\n",
    "    print(\n",
    "            \"\\t[Epochs %d/%d] [epoch loss: %6.5f] [Time:%6.5f]\"\n",
    "            % (epoch+1,bpr_config.epochs, loss_epoch, time_end - time_start)\n",
    "    ) \n",
    "    with torch.no_grad():\n",
    "        res = evaluator.top_k_evaluation(model, [3,5,10])    \n",
    "        ndcg3, precision3, hit3, map3, mrr3 = res[0]\n",
    "        ndcg5, precision5, hit5, map5, mrr5 = res[1]\n",
    "        ndcg10, precision10, hit10, map10, mrr10 = res[2]\n",
    "        writer.add_scalar('Loss', loss_epoch,epoch)\n",
    "        writer.add_scalar('Metrics/NDCG@3', ndcg3,epoch)\n",
    "        writer.add_scalar('Metrics/NDCG@5', ndcg5,epoch)\n",
    "        writer.add_scalar('Metrics/NDCG@10', ndcg10,epoch)\n",
    "        writer.add_scalar('Metrics/Precision@3', precision3,epoch)\n",
    "        writer.add_scalar('Metrics/Precision@5', precision5,epoch)\n",
    "        writer.add_scalar('Metrics/Precision@10', precision10,epoch)\n",
    "        writer.add_scalar('Metrics/Hit@3', hit3,epoch)\n",
    "        writer.add_scalar('Metrics/Hit@5', hit5,epoch)\n",
    "        writer.add_scalar('Metrics/Hit@10', hit10,epoch)\n",
    "        writer.add_scalar('Metrics/MAP', map10,epoch)\n",
    "        writer.add_scalar('Metrics/MRR', mrr10, epoch)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
